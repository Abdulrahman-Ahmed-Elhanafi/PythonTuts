{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42bc01bb",
   "metadata": {},
   "source": [
    "# <center><font size=6, color=\"#7B242F\"><u>The Theory of Extreme Gradient Boosting Machine (XGBoost)</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b9fff",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " - **Extreme Gradiend Boosting** is currently the __state-of-the-art__ algorithm for building predictive models on real-world datasets. \n",
    "    \n",
    "- Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes.\n",
    "\n",
    "- **XGboost** is a very fast, scalable implementation of gradient boosting.\n",
    "\n",
    "- Models using XGBoost regularly winning online data science competitions and being used at scale across different industries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7054e0",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19c2bd",
   "metadata": {},
   "source": [
    "## XGBoost Library\n",
    "\n",
    "- **XGBoost**: is an optimized Gradient-boosting machine learning library.\n",
    "\n",
    "- **XGBoost** is a popular machine learning library for good reasons:\n",
    "    - It was developed originally as a C++ command-line application.\n",
    "    - After winning a popular machine learning competition on **kaggle**in 2016, **Tianqi Chen** and **Carlos Guestrin** authored XGBoost: A Scalable Tree Boosting System to present their algorithm to the larger machine learning community.\n",
    "    - The adoption result of the algorithm by the ML community is bindings, or functions that tapped into the core C++ code, started appearing in a variety of other languages, including Python, R, Scala, and Julia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd7f34",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab10568",
   "metadata": {},
   "source": [
    "## The Technical Key Aspects of XGBoost: Speed\n",
    "\n",
    "- The **Extreme** in Extreme Gradient Boosting means **pushing computational limits to the extreme**. Pushing computational limits requires knowledge not just of model-building but also of disk-reading, compression, cache, and cores.\n",
    "\n",
    "The following new design features give XGBoost a big edge in speed over comparable ensemble algorithms:\n",
    "\n",
    " 1. Approximate split-finding algorithm: XGBoost presents an exact greedy algorithm in addition to a new approximate split-finding algorithm. The split-finding algorithm uses quantiles, percentages that split data, to propose candidate splits. In a global proposal, the same quantiles are used throughout the entire training, and in a local proposal, new quantiles are provided for each round of splitting.\n",
    " \n",
    " \n",
    " 2. Sparsity aware split-finding: Sparse matrices are designed to only store data points with non-zero and non-null values. This saves valuable space. A sparsity-aware split indicates that when looking for splits, XGBoost is faster because its matrices are sparse.\n",
    " \n",
    " 3. Parallel computing: it is parallelizable onto GPU's and across networks of computers, making it feasible to train models on very large datasets on the order of hundreds of millions of training examples. \n",
    " 4. Cache-aware accessL: The data on a computer is separated into **cache** and **main memory**. The cache, what we use most often, is reserved for high-speed memory.\n",
    " 5. Block compression and sharding:\n",
    "     1. **Sharding** is a method for distributing a single dataset across multiple databases, which can then be stored on multiple machines. This allows for larger datasets to be split in smaller chunks and stored in multiple data nodes, increasing the total storage capacity of the system. \n",
    "         - Block sharding decreases read times by sharding the data into multiple disks that alternate when reading the data.\n",
    "         \n",
    "    2. Block compression helps with computationally expensive disk reading by compressing columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe170ef",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0673c36",
   "metadata": {},
   "source": [
    "# XGBoost Learning Objective and Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0647cf31",
   "metadata": {},
   "source": [
    "- In order to fully understand why XGBoost is such a powerful approach to building supervised learning models (regression and classificatin), two concetps are needed to comprehend:\n",
    "    - **Loss function** also called **Learning objective function**.\n",
    "    - **Base learners or the booster**: is s the machine learning model that is constructed during every round of boosting. **gbtree** is XGBoost default base learner. Other base learner are available such as **gblinear**, **DART (Dropouts meet Multiple Additive Regression Trees)**. Check [here](https://xgboost.readthedocs.io/en/stable/tutorials/dart.html#dart-booster) to learner more about DART\n",
    "\n",
    "\n",
    "## Learning Objective \n",
    "\n",
    "  - **The learning objective or objective function** of a machine learning model determines how well the model fits the data. When we construct any machine learning model, we do so in the hopes that it minimizes the loss function across all of the data points we pass in. That's our ultimate goal, the smallest possible loss.\n",
    "  \n",
    "  \n",
    "  - In the case of XGBoost, the learning objective consists of two parts: the loss function and the regularization term.\n",
    "  \n",
    "  \n",
    "  $$\\text{obj}(\\theta) = L(\\theta) + \\Omega(\\theta)$$\n",
    "  \n",
    "  \n",
    "where $L$ is the training loss function, and $\\Omega$ is the regularization term. The training loss measures how predictive our model is with respect to the training data. \n",
    "\n",
    "A common choice of $L$ is the mean squared error, which is given by:\n",
    "\n",
    "$$L(\\theta) = \\sum_i (y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "Another commonly used loss function is logistic loss, to be used for logistic regression:\n",
    "\n",
    "$$L(\\theta) = \\sum_i[ y_i\\ln (1+e^{-\\hat{y}_i}) + (1-y_i)\\ln (1+e^{\\hat{y}_i})]$$\n",
    "\n",
    "The **regularization term** is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.\n",
    "\n",
    "For mathematical derivation, check the documentation [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html#objective-function-training-loss-regularization).\n",
    "\n",
    "\n",
    "### Common loss functions and XGBoost\n",
    "\n",
    " - **Loss functions** have specific naming conventions in XGBoost:\n",
    "      - For regression models: the most common loss function used is called **reg:linear**. \n",
    "      - For binary classification models: The most common loss functions used are\n",
    "          - **reg:logistic** when you simply want the category of the target. \n",
    "          - **binary:logistic**, when you want the actual predicted probability of the positive class. \n",
    "          - **multi:softpob**, when the dataset includes multiple classes. It computes the probabilities of classification and chooses the highest one.\n",
    "          - **reg:squarederror**: regression with squared loss.\n",
    "          \n",
    "Check [here](https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters) for a full list of objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d95609",
   "metadata": {},
   "source": [
    "## Base Learners\n",
    "\n",
    "- The **base learner** is the machine learning model **that XGBoost** uses to build the first model in its ensemble. The word **base** is used because it's the model that comes first, and the word **learner** is used because the model iterates upon itself after learning from the errors.\n",
    "\n",
    "- **XGBoost** is an **ensemble learning method** composed of many individual models that are added together to generate a single prediction. \n",
    "\n",
    "- **Base learners** are the individual models that are trained and combined in XGBoost to generate predictions.\n",
    "\n",
    "- **The goal of XGBoost** is to have base learners that is slightly better than random guessing on certain subsets of training examples, and uniformly bad at the remainder, so that when all of the predictions are combined, the uniformly bad predictions cancel out and those slightly better than chance combine into a single very good prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0c86a",
   "metadata": {},
   "source": [
    "## Installing and Importing xgboost\n",
    "\n",
    "The **xgboost** package is available at PyPI (Python Package Index), thus you can install it by using one of the following commands:\n",
    "\n",
    "```python \n",
    "# If you are using a command line use this:\n",
    "pip install xgboost \n",
    "\n",
    "# On jupyter use this\n",
    "!pip install xgboost\n",
    "\n",
    "# Or use conda manager\n",
    "conda install xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e017232",
   "metadata": {},
   "source": [
    "### Example: Trees as Base Learners with SKLearn API\n",
    "\n",
    "- Here's an example of how to train an XGBoost classification model with trees as base learners using **XGBoost's scikit-learn** compatible API. \n",
    "\n",
    "   1. Import the libraries you need \n",
    "   2. Load in the data. \n",
    "   3. Convert you data into your X matrix and y vector.\n",
    "   4. Split X and y into training and test sets.\n",
    "   5. Create your XGBoost classifier object using the **binary:logistic** objective function. (for binary classification) \n",
    "   6. Fit the classifier to the training data.\n",
    "   7. Generate predictions on the test set.\n",
    "   8. Assess the model by computing the convenient metric such as __accuracy score__.\n",
    "\n",
    "**Python syntax for training a classification model** \n",
    "```python \n",
    "# Import XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in the data (.csv for example)\n",
    "my_data = pd.read_csv('data.csv')\n",
    "\n",
    "# Create array for the features and the target\n",
    "\n",
    "X my_data.iloc[:, :-1]\n",
    "y = my_data.iloc[:, -1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size= 0.2,\n",
    "                                                    random_state=1)\n",
    "\n",
    "# Create xgboost classifier object\n",
    "xgb_clf = xgb.XGBRegressor(objective=\"binary:logistic\",\n",
    "                           n_estimators = 10, \n",
    "                           random_state = 1,\n",
    "                           eval_metric='logloss')\n",
    "\n",
    "# Fit the classifier\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(preds, y_test)\n",
    "\n",
    "# Print the metric\n",
    "print(\"The RMSE is: {:.5f}\".format(rmse))\n",
    "```\n",
    "\n",
    "Note: **XGBoost** has its own API if you wish to shift the gear and use Its native API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ff53d",
   "metadata": {},
   "source": [
    "# <center> <font color='blue'> XGBoost Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334c271",
   "metadata": {},
   "source": [
    " - Since the base learner of xgboost is **gradient boosting tree**, then all its hyperparameters are incorporated in xgboost. This makes sense, because  XGBoost is an enhanced version of gradient boosting. \n",
    " \n",
    " \n",
    " - There are other hyperparameters specific to xgboost, and they are designed to improve upon accuracy and speed.\n",
    " \n",
    " \n",
    " - Dealing with hyperparameters is an art and a science, you can't tackle them all but you should find a way to get around this process, and remember that there is **no size fits-all**. Trying different approaches is good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f061ca1",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> The list of XGBoost Hyperparameters\n",
    "_______\n",
    "    \n",
    "  - Learning any software is tiresome at the beginning, even you might feel frustrated at first, however, there are some techniques might help to solve this issue. Personally, when I start learning anything new, I would like to have a broader idea or a generall overview. Learing a python module is no different; so, in future when I need something, I remember not specifically, but I could narrow my search area. I think this is better than not knowing with area to seach. Therefore, I will print all the `XGBoost` hyperparameters, so some of them will stick in my memory. \n",
    "  \n",
    "\n",
    "The next piece of code will print the default parameters of `XGBoost` classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b7f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective                : binary:logistic\n",
      "use_label_encoder        : True\n",
      "base_score               : None\n",
      "booster                  : None\n",
      "colsample_bylevel        : None\n",
      "colsample_bynode         : None\n",
      "colsample_bytree         : None\n",
      "enable_categorical       : False\n",
      "gamma                    : None\n",
      "gpu_id                   : None\n",
      "importance_type          : None\n",
      "interaction_constraints  : None\n",
      "learning_rate            : None\n",
      "max_delta_step           : None\n",
      "max_depth                : None\n",
      "min_child_weight         : None\n",
      "missing                  : nan\n",
      "monotone_constraints     : None\n",
      "n_estimators             : 100\n",
      "n_jobs                   : None\n",
      "num_parallel_tree        : None\n",
      "predictor                : None\n",
      "random_state             : None\n",
      "reg_alpha                : None\n",
      "reg_lambda               : None\n",
      "scale_pos_weight         : None\n",
      "subsample                : None\n",
      "tree_method              : None\n",
      "validate_parameters      : None\n",
      "verbosity                : None\n",
      "========================================\n",
      "The number of total parameters 30\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# The list of parameters of xgboost classifier\n",
    "# --------------------------------------------\n",
    "from xgboost import XGBClassifier\n",
    "xgb_obj = XGBClassifier()\n",
    "for param, val in xgb_obj.get_params().items():\n",
    "    print(f'{param:25}: {val}')\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"The number of total parameters\", len(xgb_obj.get_params()))\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb048b",
   "metadata": {},
   "source": [
    "### The xgboost specific parameters\n",
    "\n",
    "- It is also possible to get hyperparameters that unique to `xgboost`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aec614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective                : binary:logistic\n",
      "base_score               : None\n",
      "booster                  : None\n",
      "colsample_bylevel        : None\n",
      "colsample_bynode         : None\n",
      "colsample_bytree         : None\n",
      "gamma                    : None\n",
      "gpu_id                   : None\n",
      "interaction_constraints  : None\n",
      "learning_rate            : None\n",
      "max_delta_step           : None\n",
      "max_depth                : None\n",
      "min_child_weight         : None\n",
      "monotone_constraints     : None\n",
      "n_jobs                   : None\n",
      "num_parallel_tree        : None\n",
      "predictor                : None\n",
      "random_state             : None\n",
      "reg_alpha                : None\n",
      "reg_lambda               : None\n",
      "scale_pos_weight         : None\n",
      "subsample                : None\n",
      "tree_method              : None\n",
      "validate_parameters      : None\n",
      "verbosity                : None\n",
      "========================================\n",
      "The number of xgboost parameters 30\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for param, val in xgb_obj.get_xgb_params().items():\n",
    "    print(f'{param:25}: {val}')\n",
    "    \n",
    "print(\"=\"*40)\n",
    "print(\"The number of xgboost parameters\", len(xgb_obj.get_params()))\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282e6d4",
   "metadata": {},
   "source": [
    "# <center><font size=6, color=\"#7B242F\"><u>Classification with XGBoost: Heart Disease Project</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1659fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================================\n",
    "#            Importing the necessary modules and tools\n",
    "## ======================================================================\n",
    "\n",
    "import pandas as pd; import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n",
    "# Import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# import the metrics for classification\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "\n",
    "# Set notebook options\n",
    "# --------------------\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "%matplotlib inline\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7fb700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0    2.300      0   \n",
       "1   37    1   2       130   250    0        1      187      0    3.500      0   \n",
       "2   41    0   1       130   204    0        0      172      0    1.400      2   \n",
       "3   56    1   1       120   236    0        1      178      0    0.800      2   \n",
       "4   57    0   0       120   354    0        1      163      1    0.600      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and print the first 5 rows\n",
    "\n",
    "hd = pd.read_csv('heart_disease.csv')\n",
    "hd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98dd161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "# print the information about the data\n",
    "\n",
    "hd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bff43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missingness\n",
    "\n",
    "hd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5f8f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex           int64\n",
       "cp            int64\n",
       "trestbps      int64\n",
       "chol          int64\n",
       "fbs           int64\n",
       "restecg       int64\n",
       "thalach       int64\n",
       "exang         int64\n",
       "oldpeak     float64\n",
       "slope         int64\n",
       "ca            int64\n",
       "thal          int64\n",
       "target        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "\n",
    "hd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ea9a2",
   "metadata": {},
   "source": [
    "## <center><u><font size=6, color=\"#990099\">Training XGBoost Classifier</font></u> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a6707",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b88133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "\n",
    "X = hd.iloc[:, :-1]\n",
    "y = hd.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97eab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate xgboost classifier with\n",
    "#        booster = \"gbtree\"\n",
    "#        objective='binary:logistic'\n",
    "#        eval_metric='logloss'\n",
    "#--------------------------------------\n",
    "\n",
    "xgb_clf = XGBClassifier(booster='gbtree', \n",
    "                      objective='binary:logistic',\n",
    "                      random_state=101, \n",
    "                      use_label_encoder=False,\n",
    "                      eval_metric = 'logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5ae59",
   "metadata": {},
   "source": [
    "Although XGBClassifier includes these values by default, we include them here to gain familiarity in preparation of modifying them in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c6d3a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "The accuracy score : 0.852\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# Fit the classifier\n",
    "# ------------------\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "# ---------------------\n",
    "pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Score the results\n",
    "# -----------------\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(\"*\"*30)\n",
    "print('The accuracy score : {:.3f}'.format(score))\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f05cb",
   "metadata": {},
   "source": [
    "# <center><u> Using Cross Validation</u>\n",
    "\n",
    "- Testing the model only one fold may give us missleading results. Thus, using cross validation is a better choice specifically in case of small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac0cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Accuracy: [0.84 0.85 0.82 0.8  0.77]\n",
      "**************************************************\n",
      "The percentage of mean accuracy: 81.497%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier using cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgb_clf,\n",
    "                         X, y, \n",
    "                         cv=5)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print('Accuracy:', np.round(cv_scores, 2))\n",
    "print(\"*\"*50)\n",
    "\n",
    "print('The percentage of mean accuracy: {:0.3f}%'.format((cv_scores.mean()*100)))\n",
    "print(\"*\"*50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531067c",
   "metadata": {},
   "source": [
    "> $\\textbf{The accuracy has gone donwn because the model has been tested on different folds.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fa61c",
   "metadata": {},
   "source": [
    "## Stratified K-Fold Cross Validation Technique\n",
    "\n",
    "- The techniques you have used for fine-tuning hyperparameters such as `GridSearchCV` and `RandomizedSearchCV` are the standard options. However, an issue  is that `cross_val_score` and `GridSearchCV/RandomizedSearchCV` do not split data the same way.\n",
    "\n",
    "\n",
    "- One solution is to use `StratifiedKFold` whenever cross-validation is used.\n",
    "\n",
    "**What is Stratified KFold**:\n",
    "\n",
    "- **A Stratified fold** includes the same percentage of target values in each fold. for example:\n",
    "    - If a dataset contains 30% positive target values (1) and 70% negative target values (0), `StratifiedKFold` ensures that each stratified test set contains 30% 1s and 70% 0s.\n",
    "    \n",
    "    - When folds are random, it's possible that one test set contains a 70- 30 split while another contains a 50-50 split of target values.\n",
    "\n",
    "**The steps of implementing StratifiedKFold**:\n",
    "\n",
    "1. Import `StratifiedKFold` from `sklearn.model_selection`:\n",
    "\n",
    "2. Create `StratifiedKFold` and define the next parameter:\n",
    "    - `n_splits`: the number of folds as kfold\n",
    "    - `shuffle`: if set to `True`, it allows rows to be initially shuffled.\n",
    "    - `random_state`: provides a consistent ordering of indices.\n",
    "    \n",
    "    \n",
    "3. To ensure consistent results, use the `StratifiedKFold` variable inside of:\n",
    "    - `cross_val_score`, or\n",
    "    - `GridSeachCV`, or \n",
    "    - `RandomizedSearchCV`.\n",
    "\n",
    "**Python syntax for implementing `StratifiedKFold`**\n",
    "```python\n",
    "# Import `StratifiedKFold`\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create kfold variable\n",
    "kfold = StratifiedKFold(n_splits=5,\n",
    "                        shuffle=True,\n",
    "                        random_state=1)\n",
    "\n",
    "# Use kfold inside cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=kfold)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40207eea",
   "metadata": {},
   "source": [
    "## `StratifiedKFold` in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c9af049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Accuracy: [0.72 0.82 0.75 0.8  0.82]\n",
      "**************************************************\n",
      "Accuracy mean: 0.782\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Implement stratified 5-fold cross validation\n",
    "# --------------------------------------------\n",
    "kfold = StratifiedKFold(n_splits=5,\n",
    "                        shuffle=True,\n",
    "                        random_state=2)\n",
    "\n",
    "scores = cross_val_score(xgb_clf,\n",
    "                         X, \n",
    "                         y,\n",
    "                         cv=kfold)\n",
    "print(\"*\"*50)\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "print(\"*\"*50)\n",
    "print('Accuracy mean: {:.3}'.format(scores.mean()))\n",
    "print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c91c2a",
   "metadata": {},
   "source": [
    "## <center> <font size=6, color=\"#7B249F\"><u>Tuning Hyperparameters using GridSearch and Random Search</u> </font>\n",
    "\n",
    "- Instead of writing two separate functions for `GridSearchCV` and `RandomizedSearchCV`, we will combine them into one one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8f3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===========================================================\n",
    "#       a standard function for hyperparameter tuning\n",
    "## ===========================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "def grid_search(params, random=False): \n",
    "    \n",
    "    xgb = XGBClassifier(booster='gbtree', \n",
    "                        objective='binary:logistic',\n",
    "                        random_state=2,\n",
    "                        use_label_encoder=False,\n",
    "                        eval_metric='logloss')\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "    \n",
    "    if random:\n",
    "        grid = RandomizedSearchCV(xgb, \n",
    "                                  params,\n",
    "                                  cv=kfold, \n",
    "                                  n_iter=20, \n",
    "                                  n_jobs=-1, \n",
    "                                  random_state=2)\n",
    "    else:\n",
    "        # Instantiate GridSearchCV as grid_reg\n",
    "        grid = GridSearchCV(xgb, \n",
    "                            params, \n",
    "                            cv=kfold,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    # Fit grid_reg on X_train and y_train\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    # Extract best params\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    # Print best params\n",
    "    print(\"Best params:\", best_params)\n",
    "    \n",
    "    # Compute best score\n",
    "    best_score = grid.best_score_\n",
    "\n",
    "    # Print best score\n",
    "    print(\"Best score: {:.5f}\".format(best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c48362",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>XGBoost Hyperparameter Fine-Tuning in Action</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0b10a",
   "metadata": {},
   "source": [
    "**A list of XGBoost hyperparameters**\n",
    "\n",
    "- $\\textbf{n_estimators}$ is the number of trees in the ensemble trained on the residuals.\n",
    "\n",
    "- $\\textbf{learning_rate, alias: eta } (\\eta)$  shrinks the weights of trees for each round of boosting. By lowering `learning_rate`, more trees are required to produce better scores. \n",
    "\n",
    "- $\\textbf{lambda}$ is the name for **l2** regularization. **L2** regularization is a much smoother penalty that **l1** and causes leaf weights to smoothly decrease, instead of enforcing strong sparsity constraints on the leaf weights as in **l1**.\n",
    "\n",
    "\n",
    "- $\\textbf{alpha}$: it is the name for L1 regularization. this regularization term is a penalty on leaf weights rather than on feature weights, as is the case in linear or logistic regression. Higher alpha values lead to stronger L1 regularization, which causes many leaf weights in the base learners to go to 0. \n",
    "\n",
    "\n",
    "- $\\textbf{gamma alias: min_split_loss}$ Known as a Lagrange multiplier, gamma provides a threshold that nodes must surpass before making further splits according to the loss function. There is no upper limit to the value of gamma, higher values lead to fewer splits. The default is 0, and anything over 10 is considered very high.\n",
    "\n",
    "\n",
    "- $\\textbf{max_depth}$ this must a positive integer value and affects how deeply each tree is allowed to grow during any given boosting round. \n",
    "\n",
    "\n",
    "- $\\textbf{min_child_weight}$ The minimum sum of weights required for a node to split. default is 1. Increasing this hyperparameter prevents overfitting. \n",
    "\n",
    "\n",
    "- $\\textbf{subsample}$ Limits the percentage of training rows used in each boosting round. Deceasing prevent overfitting. (default is 1) \n",
    "\n",
    "\n",
    "- $\\textbf{colsample_bytree}$ Limits the percentage of training columns used in each boosting round. Deceasing prevent overfitting. (default is 1) \n",
    "\n",
    "\n",
    "- $\\textbf{colsample_bylevel}$ Limits the percentage of training columns used for each depth level of the tree. Deceasing prevent overfitting. (default is 1) \n",
    "\n",
    "\n",
    "- $\\textbf{colsample_bynode}$ Limits the percentage of training columns to evaluate splits. Deceasing prevent overfitting. \n",
    "\n",
    "\n",
    "- $\\textbf{scale_pos_weight}$ used for unbalanced dataset. Default is 1.  \n",
    "\n",
    "\n",
    "- $\\textbf{max_delta_step}$ This is used fo highly unbalanced data.\n",
    "\n",
    "\n",
    "- $\\textbf{missing}$ this helps in finding optimal missing values. You can replace missing data with a a numeric number such as -999."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c7305",
   "metadata": {},
   "source": [
    "# <center><font size=6, color=\"#7B249F\"><u>Hyperparameter Fine-Tuning (One Hyperparameter at a Time Approach)</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b1586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d194ca21",
   "metadata": {},
   "source": [
    "> $\\color{blue}{\\textbf{Fine-tuning hyperparameters is an art and a science. As with both disciplines, varied approaches work.}}$\n",
    "\n",
    "- We are going to use the previously defined function `gridsearch()` to tune one hyperparameter at a time in two steps:\n",
    "   - **Step 01**: Fine Tuning each hyperparameter separately and using the defaults of other hyperparameters. \n",
    "   - **Step 02**: Using the best found hyperparameter value in the first step and start combining hyperparameters to find best combinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb166d",
   "metadata": {},
   "source": [
    "## `n_estimators`\n",
    "\n",
    "- **n_estimators**: provides the number of trees in the ensemble. In the case of XGBoost, n_estimators is the number of trees trained on the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d917db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 50}\n",
      "Best score: 0.78907\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'n_estimators':[2, 25, 50, 100, 200]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f43a2",
   "metadata": {},
   "source": [
    "## `learning_rate`\n",
    "\n",
    "`learning_rate` shrinks the weights of trees for each round of boosting. By lowering learning_rate, more trees are required to produce better scores. Lowering learning_rate prevents overfitting because the size of the weights carried forward is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6af1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.05}\n",
      "Best score: 0.79585\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ace8e",
   "metadata": {},
   "source": [
    "## `max_depth`\n",
    "\n",
    "- **max_depth** determines the length of the tree, equivalent to the number of rounds of splitting. Limiting `max_depth` prevents overfitting because the individual trees can only grow as far as `max_depth` allows. XGBoost provides a default max_depth value of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c11b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 2}\n",
      "Best score: 0.79902\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'max_depth':[2, 3, 5, 6, 8]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60c529",
   "metadata": {},
   "source": [
    "## `gamma`\n",
    "\n",
    "Known as a Lagrange multiplier, gamma provides a threshold that nodes must surpass before making further splits according to the loss function. There is no upper limit to the value of gamma. The default is 0, and anything over 10 is considered very high. Increasing gamma results in a more conservative model:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176f17d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gamma': 0.5}\n",
      "Best score: 0.79574\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'gamma':[0, 0.1, 0.5, 1, 2, 5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b388b85",
   "metadata": {},
   "source": [
    "## `min_child_weight`\n",
    "\n",
    "`min_child_weight` refers to the minimum sum of weights required for a node to split into a child. If the sum of the weights is less than the value of `min_child_weight`, no further splits are made. `min_child_weight` reduces overfitting by increasing its value:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078a68c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'min_child_weight': 5}\n",
      "Best score: 0.81219\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'min_child_weight':[1, 2, 3, 4, 5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992650f",
   "metadata": {},
   "source": [
    "## `subsample`\n",
    "\n",
    "The subsample hyperparameter limits the percentage of training instances (rows) for each boosting round. Decreasing subsample from 100% reduces overfitting:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f0e57df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.8}\n",
      "Best score: 0.79579\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'subsample':[0.5, 0.7, 0.8, 0.9, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4472a",
   "metadata": {},
   "source": [
    "## `colsample_bytree`\n",
    "\n",
    "Similar to subsample, colsample_bytree randomly selects particular columns according to the given percentage. colsample_bytree is useful for limiting the influence of columns and reducing variance. Note that colsample_bytree takes a percentage as input, not the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d33bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bytree': 0.7}\n",
      "Best score: 0.79902\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'colsample_bytree':[0.5, 0.7, 0.8, 0.9, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c44fe",
   "metadata": {},
   "source": [
    "## `colsample_bylevel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb2420fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bylevel': 0.8}\n",
      "Best score: 0.79896\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede9428",
   "metadata": {},
   "source": [
    "## `colsample_bynode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d3c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bynode': 0.8}\n",
      "Best score: 0.79896\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'colsample_bynode': [0.5, 0.6, 0.7, 0.8, 0.9, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877e8d7",
   "metadata": {},
   "source": [
    "## Combining Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96160c21",
   "metadata": {},
   "source": [
    "### `n_estimators` and `max_depth`\n",
    "\n",
    " - The best found number of trees found above is **50**, so we use this with different values of `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32255dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'max_depth':[1, 2, 3, 4, 6, 7, 8], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c09f9",
   "metadata": {},
   "source": [
    "A limitation with the approach of keeping the top values is that we may miss out on better combinations. Perhaps `n_estimators=2` or `n_esimtators=100` gives better results in conjunction with `max_depth`. Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f73e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'max_depth':[1, 2, 3, 4, 6, 7, 8], \n",
    "                    'n_estimators':[2, 50, 100]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf98d7",
   "metadata": {},
   "source": [
    " - The results did not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708320d",
   "metadata": {},
   "source": [
    "## `learning_rate`\n",
    "\n",
    "Since `n_esimtators` is reasonably low, adjusting `learning_rate` may improve results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "527dbebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                    'max_depth':[1], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f56d8",
   "metadata": {},
   "source": [
    "`learning_rate` of 0.3 is the default. That is the reason the results didn't change. But I am inclined to add values for `max_depth` to confirm the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3d9a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                    'max_depth':[1, 2, 3, 4], \n",
    "                    'n_estimators':[50, 100, 200]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9f9b1",
   "metadata": {},
   "source": [
    "## `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75795d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'min_child_weight':[1, 2, 3, 4, 5], \n",
    "                    'max_depth':[1], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc3042",
   "metadata": {},
   "source": [
    "## `subsample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc3a2f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 1, 'n_estimators': 50, 'subsample': 1}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'max_depth':[1], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d8c21",
   "metadata": {},
   "source": [
    "## Hyperparameter adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57c49742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.5, 'max_depth': 2, 'min_child_weight': 4, 'n_estimators': 2, 'subsample': 0.9}\n",
      "Best score: 0.81224\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "                    'min_child_weight':[1, 2, 3, 4, 5], \n",
    "                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                    'max_depth':[1, 2, 3, 4, 5], \n",
    "                    'n_estimators':[2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fc665",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51623ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.6, 'n_estimators': 25, 'min_child_weight': 4, 'max_depth': 4, 'learning_rate': 0.5}\n",
      "Best score: 0.82208\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "                    'min_child_weight':[1, 2, 3, 4, 5], \n",
    "                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                    'max_depth':[1, 2, 3, 4, 5, None], \n",
    "                    'n_estimators':[2, 25, 50, 75, 100]},\n",
    "            random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140e8a1",
   "metadata": {},
   "source": [
    "## `colsample_bytree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a55e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bytree': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'colsample_bytree':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'max_depth':[1], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88873a",
   "metadata": {},
   "source": [
    "## `colsample_bylevel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1874fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bylevel': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.83869\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "                    'max_depth':[1], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1915ed",
   "metadata": {},
   "source": [
    "## `colsample_bynode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4e3b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bylevel': 0.9, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.84852\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'colsample_bynode': [0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "                    'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "                    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "                    'max_depth':[1], \n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc0200",
   "metadata": {},
   "source": [
    "## `gamma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7b01b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'colsample_bylevel': 0.9, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'gamma': 0, 'max_depth': 1, 'n_estimators': 50}\n",
      "Best score: 0.84852\n"
     ]
    }
   ],
   "source": [
    "grid_search(params={'gamma':[0, 0.01, 0.05, 0.1, 0.5, 1, 2, 3],\n",
    "                    'colsample_bylevel':[0.9],\n",
    "                    'colsample_bytree':[0.8],\n",
    "                    'colsample_bynode':[0.5], \n",
    "                    'max_depth':[1],\n",
    "                    'n_estimators':[50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2eed38",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Combining All Hyperparameters at Once using Random Search</u> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70b1dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Time: 110.4425778388977 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "    \n",
    "xgb_clf = XGBClassifier(booster='gbtree', \n",
    "                    objective='binary:logistic',\n",
    "                    random_state=2,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='logloss')\n",
    "    \n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "    \n",
    "\n",
    "params={'n_estimators':[2, 25, 50, 75, 100, 200],\n",
    "        'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        'max_depth':[1, 2, 3, 4, None],\n",
    "        'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "        'min_child_weight':[1, 2, 3, 4, 5],\n",
    "        'colsample_bynode': [0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "        'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n",
    "        'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "        'gamma':[0, 0.01, 0.05, 0.1, 0.5, 1, 2, 3],\n",
    "        'alpha': [1, 10, 50, 100],\n",
    "        'lambda': [1, 10, 50, 100]}\n",
    "\n",
    "\n",
    "rs_grid = RandomizedSearchCV(estimator=xgb_clf, \n",
    "                            param_distributions=params,\n",
    "                            cv=kfold, \n",
    "                            n_iter=500, \n",
    "                            n_jobs=-1, \n",
    "                            random_state=2)\n",
    "\n",
    "# Fit grid_reg on X_train and y_train\n",
    "# -----------------------------------\n",
    "rs_grid.fit(X, y)\n",
    "      \n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print('\\nRun Time: ' + str(elapsed) + ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da0e466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample          : 0.5\n",
      "n_estimators       : 25\n",
      "min_child_weight   : 5\n",
      "max_depth          : 4\n",
      "learning_rate      : 0.05\n",
      "lambda             : 1\n",
      "gamma              : 1\n",
      "colsample_bytree   : 0.7\n",
      "colsample_bynode   : 0.5\n",
      "colsample_bylevel  : 0.5\n",
      "alpha              : 1\n"
     ]
    }
   ],
   "source": [
    "# Print best params\n",
    "# -----------------\n",
    "\n",
    "for param, val in rs_grid.best_params_.items():\n",
    "    print(\"{:18} : {}\".format(param, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26ec7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.8417\n"
     ]
    }
   ],
   "source": [
    "# Print best score\n",
    "# -----------------\n",
    "print(\"Training score: {:.4f}\".format(rs_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c58d63",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Saving Model</u> </font>\n",
    "    \n",
    " - Saving python objects in binary format is called **serializing**. `Pickle` library is a condidate library for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ec40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "final_model = XGBClassifier(booster=\"gbtree\",\n",
    "                            objective =\"binary:logistic\",\n",
    "                            n_estimators = 25, \n",
    "                            learning_rate = 0.05,\n",
    "                            min_child_weight = 5,\n",
    "                            max_depth = 4,\n",
    "                            colsample_bytree = 0.7,\n",
    "                            colsample_bynode = 0.5,\n",
    "                            colsample_bylevel = 0.5,\n",
    "                           eval_metric='logloss')\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(final_model, open(\"heart_diseaze_model.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa85f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from: heart_diseaze_model.dat\n"
     ]
    }
   ],
   "source": [
    "# Load the model when you want to use it\n",
    "loaded_model = pickle.load(open(\"heart_diseaze_model.dat\", \"rb\"))\n",
    "print(\"Loaded model from: heart_diseaze_model.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a69e9830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.11%\n"
     ]
    }
   ],
   "source": [
    "# make predictions for test data\n",
    "predictions = loaded_model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab6642",
   "metadata": {},
   "source": [
    "## <center><font size=6, color=\"#7B249F\"><u>Early stopping</u> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a4d6d",
   "metadata": {},
   "source": [
    " - **Early stopping** is a general method to limit the number of training rounds in iterative machine learning algorithms. \n",
    "\n",
    "Early stopping provides a limit to the number of rounds that iterative machine learning algorithms train on. Instead of predefining the number of training rounds, early stopping allows training to continue until n consecutive rounds fail to produce any gains, where n is a number decided by the user.\n",
    "\n",
    "- In this section, we look at three things to apply early stopping which are:\n",
    "    1. `eval_set`\n",
    "    2. `eval_metric`.\n",
    "    3. `early_stopping_rounds`.\n",
    "\n",
    "For XGBoost, `early_stopping_rounds` is the key parameter for applying early stopping. If early_stopping_rounds=10, the model will stop training after 10 consecutive training rounds fail to improve the model. Similarly, if early_stopping_rounds=100, training continues until 100 consecutive rounds fail to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1662a",
   "metadata": {},
   "source": [
    "## `eval_set` and `eval_metric`\n",
    "\n",
    "Note that `early_stopping_rounds` is not a **hyperparameter**, but a **strategy for optimizing** the `n_estimators` hyperparameter.\n",
    "\n",
    "- Normally when choosing hyperparameters, a test score is given after all boosting rounds are complete. To use early stopping, we need a test score after each round.\n",
    "\n",
    "    - `eval_metric` and `eval_set` may be used as parameters for `.fit` to generate test scores for each training round.\n",
    "\n",
    "    - `eval_metric` provides the scoring method, commonly 'error' for classification, and 'rmse' for regression. \n",
    "\n",
    "    - `eval_set` provides the test to be evaluated, commonly X_test and y_test.\n",
    "    \n",
    "\n",
    "**Syntax for applying early stopping with eval_metric and eval_set**\n",
    "```python\n",
    "\n",
    "# 1. Split the data into training and test sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# 2. Initialize the model:\n",
    "\n",
    "model = XGBClassifier(booster='gbtree', objective='binary:logistic',\n",
    "      random_state=1)\n",
    "\n",
    "# 3. Declare eval_set:\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "# 4. Declare eval_metric: \n",
    "eval_metric = 'error'\n",
    "\n",
    "# 5. Fit the model with eval_metric and eval_set:\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set)\n",
    "\n",
    "# 6. Check the final score:\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bd43d",
   "metadata": {},
   "source": [
    "## Metric evaluation Example:\n",
    "\n",
    "In this example we show how to use evaluation metric for each round of training with the default `n_estimators=100`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cddde6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "# -----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "# -----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2354538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# --------------------\n",
    "xgb_clf = XGBClassifier(booster='gbtree',\n",
    "                      objective='binary:logistic', \n",
    "                      random_state=2,\n",
    "                      eval_metric = 'logloss',\n",
    "                      use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05448883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.56653\n",
      "[1]\tvalidation_0-logloss:0.47129\n",
      "[2]\tvalidation_0-logloss:0.42604\n",
      "[3]\tvalidation_0-logloss:0.39917\n",
      "[4]\tvalidation_0-logloss:0.37390\n",
      "[5]\tvalidation_0-logloss:0.36484\n",
      "[6]\tvalidation_0-logloss:0.34916\n",
      "[7]\tvalidation_0-logloss:0.34155\n",
      "[8]\tvalidation_0-logloss:0.33709\n",
      "[9]\tvalidation_0-logloss:0.33367\n",
      "[10]\tvalidation_0-logloss:0.34333\n",
      "[11]\tvalidation_0-logloss:0.33991\n",
      "[12]\tvalidation_0-logloss:0.35260\n",
      "[13]\tvalidation_0-logloss:0.35725\n",
      "[14]\tvalidation_0-logloss:0.35855\n",
      "[15]\tvalidation_0-logloss:0.36485\n",
      "[16]\tvalidation_0-logloss:0.36139\n",
      "[17]\tvalidation_0-logloss:0.36801\n",
      "[18]\tvalidation_0-logloss:0.36803\n",
      "[19]\tvalidation_0-logloss:0.37295\n",
      "[20]\tvalidation_0-logloss:0.38163\n",
      "[21]\tvalidation_0-logloss:0.38881\n",
      "[22]\tvalidation_0-logloss:0.38720\n",
      "[23]\tvalidation_0-logloss:0.39300\n",
      "[24]\tvalidation_0-logloss:0.39693\n",
      "[25]\tvalidation_0-logloss:0.40136\n",
      "[26]\tvalidation_0-logloss:0.40828\n",
      "[27]\tvalidation_0-logloss:0.41542\n",
      "[28]\tvalidation_0-logloss:0.41555\n",
      "[29]\tvalidation_0-logloss:0.41083\n",
      "[30]\tvalidation_0-logloss:0.41473\n",
      "[31]\tvalidation_0-logloss:0.41510\n",
      "[32]\tvalidation_0-logloss:0.41698\n",
      "[33]\tvalidation_0-logloss:0.41927\n",
      "[34]\tvalidation_0-logloss:0.42231\n",
      "[35]\tvalidation_0-logloss:0.42425\n",
      "[36]\tvalidation_0-logloss:0.42602\n",
      "[37]\tvalidation_0-logloss:0.42591\n",
      "[38]\tvalidation_0-logloss:0.43254\n",
      "[39]\tvalidation_0-logloss:0.43098\n",
      "[40]\tvalidation_0-logloss:0.43329\n",
      "[41]\tvalidation_0-logloss:0.43923\n",
      "[42]\tvalidation_0-logloss:0.43645\n",
      "[43]\tvalidation_0-logloss:0.43763\n",
      "[44]\tvalidation_0-logloss:0.43717\n",
      "[45]\tvalidation_0-logloss:0.43902\n",
      "[46]\tvalidation_0-logloss:0.44383\n",
      "[47]\tvalidation_0-logloss:0.44620\n",
      "[48]\tvalidation_0-logloss:0.45044\n",
      "[49]\tvalidation_0-logloss:0.45240\n",
      "[50]\tvalidation_0-logloss:0.45278\n",
      "[51]\tvalidation_0-logloss:0.45585\n",
      "[52]\tvalidation_0-logloss:0.45645\n",
      "[53]\tvalidation_0-logloss:0.45961\n",
      "[54]\tvalidation_0-logloss:0.46243\n",
      "[55]\tvalidation_0-logloss:0.46286\n",
      "[56]\tvalidation_0-logloss:0.46443\n",
      "[57]\tvalidation_0-logloss:0.46597\n",
      "[58]\tvalidation_0-logloss:0.47003\n",
      "[59]\tvalidation_0-logloss:0.46926\n",
      "[60]\tvalidation_0-logloss:0.47415\n",
      "[61]\tvalidation_0-logloss:0.47557\n",
      "[62]\tvalidation_0-logloss:0.47687\n",
      "[63]\tvalidation_0-logloss:0.47714\n",
      "[64]\tvalidation_0-logloss:0.47772\n",
      "[65]\tvalidation_0-logloss:0.47900\n",
      "[66]\tvalidation_0-logloss:0.48164\n",
      "[67]\tvalidation_0-logloss:0.48794\n",
      "[68]\tvalidation_0-logloss:0.48773\n",
      "[69]\tvalidation_0-logloss:0.48899\n",
      "[70]\tvalidation_0-logloss:0.49153\n",
      "[71]\tvalidation_0-logloss:0.49084\n",
      "[72]\tvalidation_0-logloss:0.49008\n",
      "[73]\tvalidation_0-logloss:0.49043\n",
      "[74]\tvalidation_0-logloss:0.49110\n",
      "[75]\tvalidation_0-logloss:0.49052\n",
      "[76]\tvalidation_0-logloss:0.49300\n",
      "[77]\tvalidation_0-logloss:0.49340\n",
      "[78]\tvalidation_0-logloss:0.49327\n",
      "[79]\tvalidation_0-logloss:0.49565\n",
      "[80]\tvalidation_0-logloss:0.49750\n",
      "[81]\tvalidation_0-logloss:0.49502\n",
      "[82]\tvalidation_0-logloss:0.49187\n",
      "[83]\tvalidation_0-logloss:0.49104\n",
      "[84]\tvalidation_0-logloss:0.49423\n",
      "[85]\tvalidation_0-logloss:0.49760\n",
      "[86]\tvalidation_0-logloss:0.50092\n",
      "[87]\tvalidation_0-logloss:0.50056\n",
      "[88]\tvalidation_0-logloss:0.49977\n",
      "[89]\tvalidation_0-logloss:0.50288\n",
      "[90]\tvalidation_0-logloss:0.50459\n",
      "[91]\tvalidation_0-logloss:0.50627\n",
      "[92]\tvalidation_0-logloss:0.50824\n",
      "[93]\tvalidation_0-logloss:0.50973\n",
      "[94]\tvalidation_0-logloss:0.50867\n",
      "[95]\tvalidation_0-logloss:0.50867\n",
      "[96]\tvalidation_0-logloss:0.51079\n",
      "[97]\tvalidation_0-logloss:0.50857\n",
      "[98]\tvalidation_0-logloss:0.51036\n",
      "[99]\tvalidation_0-logloss:0.51034\n",
      "Accuracy: 82.89%\n"
     ]
    }
   ],
   "source": [
    "#  Declare eval_set\n",
    "#  ---------------\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "# Declare eval_metric\n",
    "# -------------------\n",
    "eval_metric='logloss'\n",
    "\n",
    "# Train the classifier\n",
    "# -------------------\n",
    "xgb_clf.fit(X_train, y_train,\n",
    "          eval_metric=eval_metric,\n",
    "          eval_set=eval_set)\n",
    "\n",
    "# make predictions for test data\n",
    "# ------------------------------\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "# ---------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961fa5f",
   "metadata": {},
   "source": [
    "## Using `early_stopping_rounds`\n",
    "\n",
    "`early_stopping_rounds` is an optional parameter to include with `eval_metric` and `eval_set` when fitting a model.\n",
    "\n",
    "Let's try `early_stopping_rounds=10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89efc9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.15790\n",
      "[1]\tvalidation_0-error:0.10526\n",
      "[2]\tvalidation_0-error:0.11842\n",
      "[3]\tvalidation_0-error:0.13158\n",
      "[4]\tvalidation_0-error:0.11842\n",
      "[5]\tvalidation_0-error:0.14474\n",
      "[6]\tvalidation_0-error:0.14474\n",
      "[7]\tvalidation_0-error:0.14474\n",
      "[8]\tvalidation_0-error:0.14474\n",
      "[9]\tvalidation_0-error:0.14474\n",
      "[10]\tvalidation_0-error:0.14474\n",
      "Accuracy: 89.47%\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "# -----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=2)\n",
    "\n",
    "# Instantiate the model\n",
    "# --------------------\n",
    "xgb_clf = XGBClassifier(booster='gbtree',\n",
    "                      objective='binary:logistic', \n",
    "                      random_state=2,\n",
    "                      eval_metric = 'logloss',\n",
    "                      use_label_encoder=False)\n",
    "#  Declare eval_set\n",
    "#  ---------------\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "# Declare eval_metric\n",
    "# -------------------\n",
    "eval_metric='error'\n",
    "# Train the classifier with early stopping rounds\n",
    "# -------------------\n",
    "xgb_clf.fit(X_train, y_train,\n",
    "          eval_metric=eval_metric,\n",
    "          eval_set=eval_set,\n",
    "           early_stopping_rounds=10)\n",
    "\n",
    "# make predictions for test data\n",
    "# ------------------------------\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "# ---------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c9c37",
   "metadata": {},
   "source": [
    "## A Practical Way \n",
    "\n",
    "using more boosters with more rounds for **early_stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ecb9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.15790\n",
      "[1]\tvalidation_0-error:0.10526\n",
      "[2]\tvalidation_0-error:0.11842\n",
      "[3]\tvalidation_0-error:0.13158\n",
      "[4]\tvalidation_0-error:0.11842\n",
      "[5]\tvalidation_0-error:0.14474\n",
      "[6]\tvalidation_0-error:0.14474\n",
      "[7]\tvalidation_0-error:0.14474\n",
      "[8]\tvalidation_0-error:0.14474\n",
      "[9]\tvalidation_0-error:0.14474\n",
      "[10]\tvalidation_0-error:0.14474\n",
      "[11]\tvalidation_0-error:0.15790\n",
      "[12]\tvalidation_0-error:0.15790\n",
      "[13]\tvalidation_0-error:0.17105\n",
      "[14]\tvalidation_0-error:0.17105\n",
      "[15]\tvalidation_0-error:0.17105\n",
      "[16]\tvalidation_0-error:0.15790\n",
      "[17]\tvalidation_0-error:0.17105\n",
      "[18]\tvalidation_0-error:0.15790\n",
      "[19]\tvalidation_0-error:0.17105\n",
      "[20]\tvalidation_0-error:0.17105\n",
      "[21]\tvalidation_0-error:0.17105\n",
      "[22]\tvalidation_0-error:0.18421\n",
      "[23]\tvalidation_0-error:0.18421\n",
      "[24]\tvalidation_0-error:0.17105\n",
      "[25]\tvalidation_0-error:0.18421\n",
      "[26]\tvalidation_0-error:0.18421\n",
      "[27]\tvalidation_0-error:0.18421\n",
      "[28]\tvalidation_0-error:0.18421\n",
      "[29]\tvalidation_0-error:0.18421\n",
      "[30]\tvalidation_0-error:0.18421\n",
      "[31]\tvalidation_0-error:0.18421\n",
      "[32]\tvalidation_0-error:0.18421\n",
      "[33]\tvalidation_0-error:0.18421\n",
      "[34]\tvalidation_0-error:0.18421\n",
      "[35]\tvalidation_0-error:0.18421\n",
      "[36]\tvalidation_0-error:0.18421\n",
      "[37]\tvalidation_0-error:0.18421\n",
      "[38]\tvalidation_0-error:0.18421\n",
      "[39]\tvalidation_0-error:0.18421\n",
      "[40]\tvalidation_0-error:0.18421\n",
      "[41]\tvalidation_0-error:0.18421\n",
      "[42]\tvalidation_0-error:0.18421\n",
      "[43]\tvalidation_0-error:0.17105\n",
      "[44]\tvalidation_0-error:0.18421\n",
      "[45]\tvalidation_0-error:0.17105\n",
      "[46]\tvalidation_0-error:0.18421\n",
      "[47]\tvalidation_0-error:0.18421\n",
      "[48]\tvalidation_0-error:0.17105\n",
      "[49]\tvalidation_0-error:0.15790\n",
      "[50]\tvalidation_0-error:0.17105\n",
      "[51]\tvalidation_0-error:0.17105\n",
      "[52]\tvalidation_0-error:0.15790\n",
      "[53]\tvalidation_0-error:0.17105\n",
      "[54]\tvalidation_0-error:0.17105\n",
      "[55]\tvalidation_0-error:0.17105\n",
      "[56]\tvalidation_0-error:0.17105\n",
      "[57]\tvalidation_0-error:0.17105\n",
      "[58]\tvalidation_0-error:0.17105\n",
      "[59]\tvalidation_0-error:0.17105\n",
      "[60]\tvalidation_0-error:0.17105\n",
      "[61]\tvalidation_0-error:0.17105\n",
      "[62]\tvalidation_0-error:0.17105\n",
      "[63]\tvalidation_0-error:0.17105\n",
      "[64]\tvalidation_0-error:0.17105\n",
      "[65]\tvalidation_0-error:0.17105\n",
      "[66]\tvalidation_0-error:0.18421\n",
      "[67]\tvalidation_0-error:0.18421\n",
      "[68]\tvalidation_0-error:0.18421\n",
      "[69]\tvalidation_0-error:0.18421\n",
      "[70]\tvalidation_0-error:0.18421\n",
      "[71]\tvalidation_0-error:0.18421\n",
      "[72]\tvalidation_0-error:0.18421\n",
      "[73]\tvalidation_0-error:0.18421\n",
      "[74]\tvalidation_0-error:0.17105\n",
      "[75]\tvalidation_0-error:0.18421\n",
      "[76]\tvalidation_0-error:0.17105\n",
      "[77]\tvalidation_0-error:0.18421\n",
      "[78]\tvalidation_0-error:0.15790\n",
      "[79]\tvalidation_0-error:0.17105\n",
      "[80]\tvalidation_0-error:0.15790\n",
      "[81]\tvalidation_0-error:0.15790\n",
      "[82]\tvalidation_0-error:0.15790\n",
      "[83]\tvalidation_0-error:0.15790\n",
      "[84]\tvalidation_0-error:0.15790\n",
      "[85]\tvalidation_0-error:0.15790\n",
      "[86]\tvalidation_0-error:0.15790\n",
      "[87]\tvalidation_0-error:0.15790\n",
      "[88]\tvalidation_0-error:0.15790\n",
      "[89]\tvalidation_0-error:0.15790\n",
      "[90]\tvalidation_0-error:0.15790\n",
      "[91]\tvalidation_0-error:0.15790\n",
      "[92]\tvalidation_0-error:0.15790\n",
      "[93]\tvalidation_0-error:0.17105\n",
      "[94]\tvalidation_0-error:0.17105\n",
      "[95]\tvalidation_0-error:0.17105\n",
      "[96]\tvalidation_0-error:0.17105\n",
      "[97]\tvalidation_0-error:0.17105\n",
      "[98]\tvalidation_0-error:0.17105\n",
      "[99]\tvalidation_0-error:0.17105\n",
      "[100]\tvalidation_0-error:0.17105\n",
      "Accuracy: 89.47%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "# --------------------\n",
    "xgb_clf = XGBClassifier(booster='gbtree',\n",
    "                       n_estimators = 5000,\n",
    "                       objective='binary:logistic', \n",
    "                       random_state=2,\n",
    "                       eval_metric = 'logloss',\n",
    "                       use_label_encoder=False)\n",
    "#  Declare eval_set\n",
    "#  ---------------\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "# Declare eval_metric\n",
    "# -------------------\n",
    "eval_metric='error'\n",
    "# Train the classifier with early stopping rounds\n",
    "# -------------------\n",
    "xgb_clf.fit(X_train, y_train,\n",
    "          eval_metric=eval_metric,\n",
    "          eval_set=eval_set,\n",
    "          early_stopping_rounds=100)\n",
    "\n",
    "# make predictions for test data\n",
    "# ------------------------------\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "# ---------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db5b2672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the errors\n",
    "# ------------------\n",
    "errors = xgb_clf.evals_result_.get('validation_0').get('error')\n",
    "plt.plot(errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87eb7dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best iteration\n",
    "xgb_clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d84c6c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the best ntree_limit\n",
    "xgb_clf.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49b186fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.105263"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the best score\n",
    "xgb_clf.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea873a2",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- XGBoost API [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)\n",
    "\n",
    "- SKlearn API for XGBoost [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\n",
    "\n",
    "- XGBoost github [here](https://github.com/dmlc/xgboost/blob/master/demo/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
