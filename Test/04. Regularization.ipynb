{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0de442",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea886277",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590d351",
   "metadata": {},
   "source": [
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values to be far away from the actual values. \n",
    "\n",
    "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where independent variables are highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322c652",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a4bd6",
   "metadata": {},
   "source": [
    "### The OLS Estimator \n",
    "\n",
    "$$\\boldsymbol{{\\widehat {\\beta }}}=\\mathbf{\\Bigg(X^{T}X \\Bigg)^{-1}X^{T}y}$$\n",
    "\n",
    "where ${\\textstyle X^{T}}$ is the transpose of ${\\textstyle X}$.\n",
    "### The Cost Function of OLS \n",
    "\n",
    "$$Cost_{ols}=\\textbf{RSS}=\\boldsymbol{\\sum_{i=1}^{n} \\Bigg (y_{i} - \\beta_{0} - \\sum_{j=i}^{p}\\beta_{j}x_{ij} \\Bigg)^{2}}$$\n",
    "\n",
    "### The Ridge Estimator \n",
    "\n",
    "$$\\boldsymbol{{\\widehat {\\beta }}_{\\text{ridge}}} = \\mathbf{\\Bigg(X^{T}X+\\lambda I_{p}\\Bigg)^{-1}X^{T}y}$$\n",
    "\n",
    "where ${\\textstyle I_{p}}$ is the ${\\textstyle p \\times p}$ identity matrix and ${\\textstyle \\lambda \\ge 0}$.\n",
    "\n",
    "### The Cost Function of Ridge Regression\n",
    "\n",
    "$$Cost_{ridge} = \\boldsymbol{\\sum_{i=1}^{n} \\Bigg (y_{i} - \\beta_{0} - \\sum_{j=i}^{p}\\beta_{j}x_{ij} \\Bigg)^{2}} + \\lambda \\sum_{j=1}^{p}\\beta_{j}^2  = \\textbf {RSS} + \\lambda \\sum_{j=1}^{p}\\beta_{j}^2$$\n",
    "\n",
    "\n",
    "where $\\boldsymbol {\\lambda \\ge 0}$ is a __**tuning parameter**__, which will be determined separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1e29b",
   "metadata": {},
   "source": [
    "### Ridge Regression Estimates \n",
    "\n",
    "1. The ridge coefficients minimize a penalized residual sum of squares (Cost_ridge)\n",
    "   \n",
    "   \n",
    "2. Ridge Regression produces a set of estimates for each $\\lambda$ as opposed to OLS regression that produces only on set of estimates.\n",
    "   \n",
    "3. Ridge has a second term $\\lambda \\sum_{j=1}^{p}\\beta_{j}^2$ called ${\\textbf{shrinkage  penalty}}$\n",
    "\n",
    "\n",
    "4. If $\\boldsymbol \\lambda = 0$, then the ridge is simply the OLS estimator (no penalty). \n",
    "\n",
    "\n",
    "5. If $\\boldsymbol \\lambda \\rightarrow + \\infty$ then the ridge goes to **0** (severe penalty, all coefficients will approach zero).\n",
    "   \n",
    "   \n",
    "6. Selecting a good $\\boldsymbol \\lambda$ value is a critical step in building machine learning models. It is often selected through a search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4daf787",
   "metadata": {},
   "source": [
    "## Standardization \n",
    "\n",
    "- In ridge regression, the first step is to standardize the variables (both dependent and independent) by subtracting their means and dividing by their standard deviations. \n",
    "\n",
    "- All ridge regression calculations are based on standardized variables. When the final regression coefficients are displayed, they are adjusted back into their original scale. However, the ridge trace is on a standardized scale.\n",
    "\n",
    "$$\\tilde x_{ij}=\\frac{x_{ij}}{\\sqrt { \\frac{1}{n} \\sum_{i=1}^{n} \\big( x_{ij} - \\bar x_{j} \\big)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10635ef8",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    " 1. in Sklearn module, $\\lambda$ is called $\\alpha$\n",
    " \n",
    " 2. Ridge Regression is said to perform $l_{2}$ **regularization**.\n",
    " \n",
    " 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f9779",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746522c",
   "metadata": {},
   "source": [
    "## Bias and variance trade-off\n",
    "\n",
    "Bias and variance trade-off is generally complicated when it comes to building ridge regression models on an actual dataset. However, following the general trend which one needs to remember is:\n",
    "\n",
    "The bias increases as λ increases.\n",
    "The variance decreases as λ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c97578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057a0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf741c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a3c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70da61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b38c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bd346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeaa676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b560585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b25bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0608db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9369e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://www.mygreatlearning.com/blog/what-is-ridge-regression/\"\n",
    "url2 = \"https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = \"https://the-learning-machine.com/article/ml/ridge-regression?gclid=Cj0KCQiA4b2MBhD2ARIsAIrcB-SrXU9dtcCHT0YvuA6Dq5X4ftxczbexYqFDtRCqKPcWg1pyQsT2jzIaAiAPEALw_wcB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b0a02",
   "metadata": {},
   "source": [
    "url4 = \"https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e9136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
